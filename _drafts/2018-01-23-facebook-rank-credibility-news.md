---
published: false
layout: post
tags:
  - fake news
  - ranking
  - facebook
  - users
---
Facebook has a fake news problem.

Almost every person, their friends, and their parents have been exposed to these. Fake news often comes in the form of clickbait-titled links, leading to [sensational articles which spread lies](https://www.engadget.com/2018/01/16/facebook-news-feed-tweak-could-make-fake-news-worse/).

# What is Facebook doing? 

According to [New York Times](https://www.nytimes.com/2018/01/19/technology/facebook-news-feed.html), it is asking users to evaluate the trustworthiness of news outlets. Data from this evaluation will be used to promote "trusted" sites over "untrusted" ones.

To me, the fake news problem has two sides:
- the evil author of the fake article or site
- the gullible reader/spreader of fake news

While the Facebook approach may tackle the evil author in some way, it is relying on data from the weaker actor of the problem: the gullible reader. 

# The cure is worse than the disease

This solution is bad on so many levels:
- fake news target gullible readers, so in principle one should not ask a deceivable person if a news outlet is trying to deceive him/her;
- the survey addresses [the whole site](http://www.telegraph.co.uk/technology/2018/01/24/facebook-defends-two-question-fake-news-survey/) and not the single news: what happens if a reputable site mistakenly bait on a fake news? I know this may sound unethical, but it happened in the past and will surely happen again;
- the readers can be targeted via facebook groups, which are not moderated via the same survey. Recent data have shown that russians have used groups to [spread fake news and bias people on religion and immigration](https://www.nytimes.com/2018/02/17/technology/indictment-russian-tech-facebook.html);
- what happens if a reputable site is targeted by a big group and purposely marked as "not trustworthy"? It may be showed less in people's timeline, losing users in the process;
- the opposite of the last point is also possible: a fake news outlet can be so popular that people start believing it's trustworthy. I don't even need to explain why this is bad;
- people in closed groups of polarized friends (i.e. echo chambers) are still vulnerable to fake news even when the trust-filter is employed correctly: facebook has to show something in your feed and [if the majority of your social friends share fake news, you get fake news anyway](https://arstechnica.com/science/2017/03/the-social-media-echo-chamber-is-real/).

# Please make it work

Facebook currently employs some of the best engineers and scientists of the world and yet it still has find no way to automatically flag content as fake. I'm not arguing it's easy, but Facebook is has [more than 2 billion users](https://www.facebook.com/zuck/posts/10104501954164561) and surely has the data to feed some finely tuned AI to do this.

Please make it work.
